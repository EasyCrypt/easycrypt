VPERMD(w@256, widx@256) -> @256 =
  map<32, 8>(
    fun idx@32 . let i = (unsigned) idx[0:2] in w[i:1:32],
    widx
  )

VPSUB_16u16(w1@256, w2@256) -> @256 =
  map<16, 16>(
    fun x@16 y@16 . sub<16>(x, y),
    w1,
    w2
  )

VPADD_16u16(w1@256, w2@256) -> @256 =
  map<16, 16>(
    fun x@16 y@16 . add<16>(x, y),
    w1,
    w2
  )

VPAND_256(w1@256, w2@256) -> @256 = 
  and<256>(w1, w2)

VPAND_256_ALT(w1@256, w2@256) -> @256 = 
  map<1, 256>(
    fun a@1 b@1 . and<1>(a, b),
    w1,
    w2
  )

# REF: https://www.felixcloutier.com/x86/vpbroadcast
# Might be problematic?
VPBROADCAST_16u16(w1@256) -> @256 = 
  map<16, 16>(
    fun x@16 . w1[0:15],
    w1
  )
    
# REF: https://www.felixcloutier.com/x86/pmaddubsw
VPMADDUBSW_256(w1@256, w2@256) -> @256 =
  
# REF: https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#techs=MMX,SSE_ALL,AVX_ALL&ig_expand=324,324,101,6663,4774&text=vpmulh
# mu
VPMULH_16u16(w1@256, w2@256) -> @256 =
  map<16, 16>(
    fun x@16 y@16 . mult<16>(x, y)[,
    w1,
    w2
  )

# Need to implement sign extend (intrinsic?)
# TODO: Check indexing here
# rshift<n>(x,y): right shift logical x by y (x >> y) as n-bit ints
# sext<n>(x): sign extend x to n bits
# Can be implemented differently from this
VPMULHRS_16u16(w1@256, w2@256) -> @256 =
  map<16, 16>(
    fun x@16 y@16 . add<32>(
                      rshift<32>(
                        mult<32>(sext<32>(x), sext<32>(y)), 
                        14),
                      1)[0:15],
    w1,
    w2)

# REF: https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#techs=MMX,SSE_ALL,AVX_ALL&ig_expand=324,324,101,6663,4774,4785,6371&text=vpsra
# TODO?: Might need to be expanded here 
# to match the intrinsics guide pseudocode at this level
# shift_rigbt_ar(x,y): shift right arithmetic, x >> y
VPSRA_16u16(w1@256, w2@256) -> @256 =
  map<16, 16>(
    fun x@16 y@16 . shift_right_ar<16>(x, y)
    w1,
    w2
  )


# SatToSW<n>: SaturateToSignedWord<bit_len>
VPMADDUBSW_256(w1@256, w2@256) -> @256 =
  map<16, 16>(
    fun x@16 y@16 . SatToSW<16>(
                      add<16>(
                        mult<8>(x[0:7], y[0:7]),
                        mult<8>(x[8:15], y[8:15])
                      )
                    ),
    w1,
    w2
  )



# REF: https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#techs=MMX,SSE_ALL,AVX_ALL&ig_expand=324,324,101,6663,4785,4906&text=vpackus
# Might need different implementation
# probably needs some kind of fold/accumulation combinator
# TODO: FINISH THIS
VPACKUS_16u16(w1@256, w2@256) -> @256 =
  map<128, 2>(
    fun v1@128 v2@128 . or<128>(
                          
    w1,
    w2,
  )
                      

## TODO:
## VPACKUS_16u16
## VPBROADCAST_16u16 *
## VPMADDUBSW_256 *
## VPMULHRS_16u16 *
## VPSRA_16u16 *
## Check for mult bit handling consistency
## Check for bit addressing order consistency (endianness)
